{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vivek-1116/SDAE-and-VAE-for-Cancer-Classification-through-Multi-omics-Feature-Extraction/blob/main/SDAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryqKLAAbq3ez"
      },
      "source": [
        "#Stacked Denoising Autoencoder Model (SDAE)\n",
        "This model includes Feature Selection (FS) technique \"Recursive Feature Elimination (RFE)\" and Class Imbalance algorithm \"Synthetic Minority Oversampling Technique (SMOTE)\" to further reduce data complexity & enhance computational efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX1vQff7rP-I"
      },
      "source": [
        "IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7L5ZP2qwjst"
      },
      "source": [
        "import numpy as np # Lib for multi-dimensional arrays and matrices handling\n",
        "import pandas as pd # Lib for data manipulation and analysis\n",
        "\n",
        "from sklearn import svm # SVM model for RFE Feature Selection\n",
        "from sklearn.preprocessing import * # Data pre-processing and standardization\n",
        "from sklearn.preprocessing import MinMaxScaler # Feature scaling\n",
        "from sklearn.pipeline import Pipeline # To fit hyperparameters into 1 pipeline\n",
        "from sklearn.model_selection import train_test_split # Splits data into indices of training and testing\n",
        "from imblearn.over_sampling import SMOTE # Oversample data using SMOTE algorithm\n",
        "\n",
        "import warnings # Lib for warning issue handling\n",
        "warnings.filterwarnings('ignore') # Ignores all irrelevant warnings\n",
        "from collections import Counter # To get / set count of elements\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt # Lib for interactive plots\n",
        "plt.style.use('seaborn-white') # Sets theme of visualization (seaborn-ticks / whitegrid) are similar to white\n",
        "import seaborn as sns # Matplotlib based lib - better interface for drawing attractive and informative statistical graphics\n",
        "sns.set_palette(['#FC4B60','#06B1F0']) \n",
        "random_seed = 63445 \n",
        "\n",
        "# Framework / Platform for building ML models\n",
        "import tensorflow \n",
        "from tensorflow.keras import regularizers \n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, Sequential, load_model, save_model\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, GaussianNoise, LeakyReLU, Dropout\n",
        "\n",
        "print(\"TensorFlow Version:\",tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1hrwhO-u74T"
      },
      "source": [
        "MOUNT DRIVE & SET DATA PATH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dd2GZC7uyLo"
      },
      "source": [
        "from google.colab import drive # Link notebook with google drive\n",
        "drive.mount('/content/gdrive/') # To retrieve data from our personal Gdrive (can remove this line if we are about to access data from pc)\n",
        "# Define path\n",
        "data_path = '/content/gdrive/My Drive/PSM2 VIVEK/LUSC Dataset/' # Change this path accordingly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zis3HwpWvHEN"
      },
      "source": [
        "CHECKING FOR MACHINE RUNTIME TYPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIhOINUKHGEK"
      },
      "source": [
        "# Checking whether your machine running only on CPU or with GPU\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "CUDA_VISIBLE_DEVICES = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIBsTHi2rZPO"
      },
      "source": [
        "IMPORT DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgB5P2QJIKhU"
      },
      "source": [
        "# Create DataFrame\n",
        "print(\"Reading Multi-omics Data\")\n",
        "df = pd.read_csv(data_path + \"Complete_MultiOmics.csv\", delimiter=\",\", index_col=0)\n",
        "nFeatures = len(df.columns) - 2 # First and last column does not hold data to be processed\n",
        "print(\"Number of features :\", nFeatures)\n",
        "print(\"Size of Dataset :\", df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NVfcMLaLU5f"
      },
      "source": [
        "DISPLAY MULTI-OMICS DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffk0NLFAWRhj"
      },
      "source": [
        "print(\"Multi-omics data imported successfully\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMIbViacsdWO"
      },
      "source": [
        "# RECURSIVE FEATURE ELIMINATION (RFE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm8qHwS0jFN-"
      },
      "source": [
        "DATA AGGREGATION (FEATURES & TARGETS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTgR2FJui8O3"
      },
      "source": [
        "features = df.iloc[:,1:-1] #Retrieves all rows (1:), leaves last column (,1:-1)\n",
        "target = df.iloc[:,-1] #Retrieves all rows (1:), retrieves only last column (,-1)\n",
        "\n",
        "print(\"Number of features :\", features.shape[1])\n",
        "print(\"Number of targets :\", target.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoEvUoA1lSpq"
      },
      "source": [
        "DATA STANDARDIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drFmDfMJlTJZ"
      },
      "source": [
        "#Setting all dataset into a range of 0 to 1\n",
        "min_max_scaler = MinMaxScaler(feature_range =(0, 1)) \n",
        "# Scaled feature \n",
        "features = pd.DataFrame(min_max_scaler.fit_transform(features)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnRHfLVQaEHb"
      },
      "source": [
        "SVM MODEL FITTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2ke9lMGaFV9"
      },
      "source": [
        "#Function to Define SVM model accuracy\n",
        "def accuracy(model, features, target):\n",
        "\tprediction = model.predict(features)\n",
        "\tprint (\"Accuracy of model:\", accuracy_score(target, prediction) * 100, \"%\")\n",
        "    \n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ct1S44oZ_lN"
      },
      "source": [
        "#Set Parameter\n",
        "C = 1.0\n",
        "rfeIndex = nFeatures\n",
        "#Create SVM model using a linear kernel\n",
        "model = svm.SVC(kernel='linear', C=C).fit(features,target)\n",
        "coef = model.coef_\n",
        "\n",
        "#Print co-efficients of features\n",
        "for i in range(0, nFeatures):\n",
        "\tprint (features.columns[i],\":\", coef[0][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtoZwt-EaUXn"
      },
      "source": [
        "RFE IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JabbAXLcVOSU"
      },
      "source": [
        "#Find the minimum weight among features and eliminate the feature with the smallest weight\n",
        "min = coef[0][0]\n",
        "\n",
        "for j in range(6663): # To make sure only 12,000 features remain\n",
        "\tindex = 0\n",
        "\tfor i in range(0,rfeIndex): # Iterates until the final feature\n",
        "\t\tif min > coef[0][i]:\n",
        "\t\t\tindex = index + 1\n",
        "\t\t\tmin = coef[0][i]\n",
        "\n",
        "\tif len(features.columns) == 1:\n",
        "\t\tprint (\"After recursive elimination we have the\", features.columns[index], \"feature with a score of:\", min)\n",
        "  \n",
        "\telse:\n",
        "\t\tprint (\"Lowest feature weight is for\", features.columns[index], \"with a value of:\", min)\n",
        "\t\tprint (\"Dropping feature\", features.columns[index])  \n",
        "\t\tfeatures.drop(features.columns[index], axis = 1, inplace = True)\n",
        "\t\trfeIndex = rfeIndex - 1\n",
        "\t\tnFeatures = nFeatures - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHEjki1daoH5"
      },
      "source": [
        "FEATURES AFTER RFE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiD_kVianHU"
      },
      "source": [
        "print(\"Dataset size after Feature Selection:\",features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i8CePxkMNpD"
      },
      "source": [
        "NORMALIZED FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkfRnRNcvPgn"
      },
      "source": [
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_doVeGygikFs"
      },
      "source": [
        "# SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE (SMOTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23dNEqMditAq"
      },
      "source": [
        "CLASS IMBALANCE COUNTER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJejLhpWieAq"
      },
      "source": [
        "from collections import Counter\n",
        "counter = Counter(df['Class'])\n",
        "counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzgam0nRivrb"
      },
      "source": [
        "CLASS IMBALANCE PLOT (VISUALIZATION)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf8N3-i_irLl"
      },
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "ax = sns.countplot(df['Class'])\n",
        "ax.set(xlabel='Class', ylabel='Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJVMrvARjN1S"
      },
      "source": [
        "DATA SPLITTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSbRZY0ejKMj"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.25, random_state=random_seed, stratify=target)\n",
        "print (\"X TRAIN DATA SHAPE: \", x_train.shape)\n",
        "print (\"X TEST DATA SHAPE: \", x_test.shape)\n",
        "print (\"Y TRAIN DATA SHAPE: \", y_train.shape)\n",
        "print (\"Y TEST DATA SHAPE: \", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tIiTqbwjl5H"
      },
      "source": [
        "SMOTE MODEL FITTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOwWF1DNjaex"
      },
      "source": [
        "sm = SMOTE(k_neighbors=1, random_state=random_seed)\n",
        "X, Y = sm.fit_sample(x_train, y_train)\n",
        "print ('Shape of oversampled data: {}'.format(X.shape))\n",
        "print ('Shape of Y: {}'.format(Y.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFmyI6vTkNi1"
      },
      "source": [
        "BALANCED DATA VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKoGanmIkQJk"
      },
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "ax = sns.countplot(Y)\n",
        "ax.set(xlabel='Class', ylabel='Frequency')\n",
        "plt.title('Balanced training data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ks5YUt93mSx"
      },
      "source": [
        "CLASS (TARGET) AFTER SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOqoMpnH3n04"
      },
      "source": [
        "print('Resampled dataset shape for Train {}'.format(Counter(Y)))\n",
        "print('Normal validation dataset shape for Test {}'.format(Counter(y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-mO_4PvsoKf"
      },
      "source": [
        "# AUTOENCODER MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMm6OBxrtf4p"
      },
      "source": [
        "PARAMETER SETTING FOR DAE MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enhlVqr8KrBm"
      },
      "source": [
        "# Stop training model when \"monitor parameter\" has stopped improving\n",
        "earlyStopping = EarlyStopping(monitor='loss', patience=50) # patience is num of epochs to reach early stopping\n",
        "# Save model after every epoch\n",
        "checkpointer = ModelCheckpoint(filepath='MO_SDAE_Training.h5', verbose=1, save_best_only=True)\n",
        "input_dims = (len(features)-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kvdFVhdkV88"
      },
      "source": [
        "DENOISING AUTOENCODER (DAE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etBocspfeoqT"
      },
      "source": [
        "def dae (inputX, input_dims, output_dims, epoch, activation, loss, opti):\n",
        "            \n",
        "    model = Sequential()\n",
        "    \n",
        "    if input_dims > 9999:\n",
        "        with tensorflow.device('/cpu:0'):\n",
        "            print(\"Using CPU....\")\n",
        "            model.add(Dense(input_dims, input_dim = input_dims))\n",
        "            model.add(GaussianNoise(0.5)) \n",
        "            model.add(Dense(output_dims, activation = activation, kernel_regularizer = regularizers.l1(0.01)))\n",
        "            model.add(Dense(input_dims, activation= activation))\n",
        "            model.compile(loss = loss, optimizer = opti)\n",
        "            model.fit(inputX, inputX, epochs = epoch, batch_size = 4)\n",
        "            model.summary()\n",
        "    else:\n",
        "        with tensorflow.device('/device:GPU:0'):\n",
        "            print(\"Using GPU....\")\n",
        "            model.add(Dense(input_dims, input_dim = input_dims))\n",
        "            model.add(GaussianNoise(0.5)) \n",
        "            model.add(Dense(output_dims, activation= activation, kernel_regularizer = regularizers.l1(0.01)))\n",
        "            model.add(Dense(input_dims, activation= activation))\n",
        "            model.compile(loss = loss, optimizer = opti)\n",
        "            model.fit(inputX, inputX, epochs = epoch, batch_size = 4)\n",
        "            model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYtJxd7Ye-Gv"
      },
      "source": [
        "autoencoder = dae(X, \n",
        "                  input_dims = 12000,\n",
        "                  output_dims = 500,\n",
        "                  epoch = 5,\n",
        "                  activation = 'relu', \n",
        "                  loss = 'mse',\n",
        "                  opti = 'adam', \n",
        "                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TsQTSkkNddu"
      },
      "source": [
        "HYPER-PARAMETER SETTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A2uKPSzkmUH"
      },
      "source": [
        "# Hyper-parameter\n",
        "layers = [12000, 10000, 8000, 6000, 4000, 2000, 500] # Setting the size of your layer \n",
        "epoch = 3 \n",
        "optimizer = 'adam'\n",
        "activation = 'relu'\n",
        "loss = 'mse'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHlOpqG0keIA"
      },
      "source": [
        "STACKED DENOISING AUTOENCODER (SDAE) - PRETRAIN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "096KlbzLkzDO"
      },
      "source": [
        "def sdae_pretrain (inputX, layers, activation, epoch, optimizer, loss):\n",
        "    \n",
        "    encoder = []\n",
        "    decoder = []\n",
        "    ae = []\n",
        "    \n",
        "    for i in range(len(layers)-1):\n",
        "            print(\"Now pretraining layer {} [{}-->{}]\".format(i+1, layers[i], layers[i+1]))\n",
        "\n",
        "            input_dims = layers[i]\n",
        "            output_dims = layers[i+1]\n",
        "            \n",
        "            autoencoder = dae(inputX, input_dims, output_dims, epoch, activation, loss, optimizer)\n",
        "            enc = Sequential()\n",
        "            enc.add(Dense(output_dims, input_dim=input_dims))\n",
        "            enc.compile(loss=loss, optimizer=optimizer) \n",
        "            enc.layers[0].set_weights(autoencoder.layers[2].get_weights())\n",
        "            inputX = enc.predict(inputX)\n",
        "            print(\"check dimension : \", inputX.shape)\n",
        "            enc.summary()\n",
        "            encoder.append(autoencoder.layers[2].get_weights())\n",
        "            decoder.append(autoencoder.layers[3].get_weights())\n",
        "            ae.append(autoencoder)\n",
        "    \n",
        "    return ae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvwr64WHk7UW"
      },
      "source": [
        "Train_SDAE = sdae_pretrain( X, layers = layers, activation = activation, epoch = epoch, optimizer = optimizer, loss = loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ay9E-BqNvlu"
      },
      "source": [
        "STORING PRE-TRAINED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO_O0frempVm"
      },
      "source": [
        "for i, m in enumerate(Train_SDAE):\n",
        "    filename=\"pretrain_model\" + str(i) + \".hd5\"\n",
        "    m.save(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEe38oYAN5ME"
      },
      "source": [
        "LOADING PRE-TRAINED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSbAJbQQnlmQ"
      },
      "source": [
        "Train_SDAE = []\n",
        "with tensorflow.device('/cpu:0'):\n",
        "    for i in range (len(layers)-1):\n",
        "      Train_SDAE.append(load_model(\"pretrain_model\"+ str(i) + \".hd5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqMJjWBXhQFy"
      },
      "source": [
        "DEFINING CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_i1-cjm7zsu"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    precision2 = precision(y_true, y_pred)\n",
        "    recall2 = recall(y_true, y_pred)\n",
        "    return 2*((precision2 * recall2)/(precision2 + recall2 + K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFuZyIjnny64"
      },
      "source": [
        "STACKED DENOISING AUTOENCODER - FINE TUNING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk36J4VUn7Gx"
      },
      "source": [
        "def fine_tuning(weights, inputX, inputXt, inputY, inputYt, layers, epoch, batch, optimizer, loss):\n",
        "\n",
        "    encoder = []\n",
        "    decoder = []\n",
        "\n",
        "    for i in range(len(Train_SDAE)):\n",
        "    \n",
        "        encoder.append(Train_SDAE[i].layers[2].get_weights())\n",
        "        decoder.append(Train_SDAE[i].layers[3].get_weights())\n",
        "    \n",
        "    with tensorflow.device('/device:gpu:0'): #I have to put this because the model size is too big for my GPU\n",
        "        ft = Sequential()\n",
        "        ft.add(Dense(layers[0], input_dim=layers[0]))\n",
        "        ft.add(GaussianNoise(0.5))\n",
        "\n",
        "        for i in range(len(layers)-1):\n",
        "            ft.add(Dense(layers[i+1], activation = 'relu', weights = encoder[i], kernel_regularizer = regularizers.l2(0.01))) # Initial regularizer (l1_l2)\n",
        "        \n",
        "        for i in reversed(range(len(layers)-1)):\n",
        "            ft.add(Dense(layers[i], activation = 'relu', weights = decoder[i]))\n",
        "    ft.add(Dropout(0.2))\n",
        "    ft.add(Dense(200, activation = 'relu'))\n",
        "    ft.add(Dense(150, activation = 'relu', use_bias=True))\n",
        "    ft.add(Dense(100, activation = 'relu', kernel_initializer= \"glorot_uniform\", bias_initializer= \"zeros\")) \n",
        "    ft.add(Dense(50, activation= 'relu', kernel_initializer= \"glorot_uniform\", bias_initializer= \"zeros\"))   \n",
        "    ft.add(Dense(1, activation = 'sigmoid')) \n",
        "    \n",
        "    ft.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', recall, precision, f1])\n",
        "    History = ft.fit(X, Y, epochs = epoch, batch_size = batch, validation_data = (x_test, y_test))\n",
        "    ft.summary()\n",
        "\n",
        "    plt.plot(History.history['loss']) \n",
        "    plt.plot(History.history['val_loss']) \n",
        "    plt.title('SDAE Model Loss') \n",
        "    plt.ylabel('Loss') \n",
        "    plt.xlabel('Epoch') \n",
        "    plt.legend(['Train', 'Test'], loc='upper right') \n",
        "    plt.show()\n",
        "\n",
        "    return ft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4l4Mc90qJr7"
      },
      "source": [
        "Result = fine_tuning(Train_SDAE, X, x_test, Y, y_test, layers = layers, epoch = 50, batch = 4, optimizer = 'adam', loss = 'binary_crossentropy') "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}